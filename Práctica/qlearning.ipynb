{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraciones\n",
    "from collections import defaultdict\n",
    "\n",
    "# Las de siempre\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Aprendizaje por Refuerzo\n",
    "import gymnasium as gym  # https://gymnasium.farama.org/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Del Punto A al Punto B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                    O----T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states = 6   # longitud del entorno\n",
    "actions = ['left', 'right']     # acciones disponibles\n",
    "epsilon = 0.9   # 'greedy policy': exploración vs explotación -> exploración (non-greedy)\n",
    "alpha = 0.1     # 'learning rate': nivel de 'confianza' del agente -> desconfiado, cauto\n",
    "gamma = 0.9    # 'discount factor': si las recompensas disminuyen con el tiempo.\n",
    "max_episodes = 13   # número máximo de episodios\n",
    "fresh_time = 0.3    # tiempo de actualización entre episodios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_q_table(n_states, actions):\n",
    "    table = pd.DataFrame(np.zeros((n_states, len(actions))),\n",
    "                         columns= actions)\n",
    "    # print(table)\n",
    "    return table\n",
    "\n",
    "build_q_table(6, ['left', 'right'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, q_table):\n",
    "    state_actions = q_table.iloc[state, :]\n",
    "    if (np.random.uniform() > epsilon) or ((state_actions == 0).all()):  # non greedy, obligándolo a explorar\n",
    "        action_name = np.random.choice(actions)\n",
    "    else:   # greedy\n",
    "        action_name = state_actions.idxmax()    # escoger la acción con el valor más alto\n",
    "    return action_name\n",
    "\n",
    "\n",
    "def get_env_feedback(S, A):\n",
    "    # Interacción del agente con el entorno\n",
    "    if A == 'right':  # Si te mueves a la derecha\n",
    "        if S == n_states - 2:  # Y estabas en el penúltimo lugar (-1 es el último)\n",
    "            S_ = 'terminal'\n",
    "            R = 1  # recompensa\n",
    "        else:\n",
    "            S_ = S + 1\n",
    "            R = 0\n",
    "    else:   # izquierda\n",
    "        R = 0\n",
    "        if S == 0:\n",
    "            S_ = S  # inicio\n",
    "        else:\n",
    "            S_ = S - 1\n",
    "    return S_, R\n",
    "\n",
    "\n",
    "def update_env(S, episode, step_counter):\n",
    "    env_list = ['-']*(n_states-1) + ['T']   # '---------T'\n",
    "    if S == 'terminal':\n",
    "        interaction = 'Episode %s: total_steps = %s' % (episode +1, step_counter)\n",
    "        print('\\r{}'.format(interaction), end='')\n",
    "        time.sleep(2)\n",
    "        print('\\r                                ', end='')\n",
    "    else:\n",
    "        env_list[S] = 'o'\n",
    "        interaction = ''.join(env_list)\n",
    "        print('\\r{}'.format(interaction), end='')\n",
    "        time.sleep(fresh_time)\n",
    "\n",
    "\n",
    "def rl():\n",
    "    # Aprendizaje por refuerzo (RL)\n",
    "    q_table = build_q_table(n_states, actions)\n",
    "    for episode in range(max_episodes):\n",
    "        step_counter = 0\n",
    "        S = 0\n",
    "        is_terminated = False\n",
    "        update_env(S, episode, step_counter)\n",
    "        while not is_terminated:\n",
    "\n",
    "            A = choose_action(S, q_table)  # Elegir A basada en S actual y QT\n",
    "            S_, R = get_env_feedback(S, A)  # Obtener nuevo S y R\n",
    "            q_predict = q_table.loc[S, A]\n",
    "            if S_ != 'terminal':\n",
    "                q_target = R + gamma * q_table.iloc[S_, :].max() # Lo que tiene y lo que quiere\n",
    "                                                                 # Al principio, no sabe lo que quiere, será todo aleatorio\n",
    "            else:\n",
    "                q_target = R    \n",
    "                is_terminated = True\n",
    "\n",
    "            q_table.loc[S, A] += alpha * (q_target - q_predict) # Ajusta QT en base a sus \"experiencias\":\n",
    "                                                                # Si la realidad es mejor de lo que esperaba, ajusta sus expectativas hacia arriba\n",
    "                                                                # Si es peor, las ajusta hacia abajo.\n",
    "            S = S_ \n",
    "\n",
    "            update_env(S, episode, step_counter+1)\n",
    "            step_counter += 1\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(5)\n",
    "\n",
    "q_table = rl()\n",
    "print('\\r\\nQ-table:\\n')\n",
    "q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gymnasium (Gym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_keys = gym.envs.registry.keys()\n",
    "\n",
    "for key in env_keys:\n",
    "    print(key)\n",
    "\n",
    "print('------------------')\n",
    "print('Total de entornos:', len(env_keys))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cartpole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blackjack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://gymnasium.farama.org/_images/blackjack_AE_loop_dark.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funcionamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make('Blackjack-v1',\n",
    "#                #nat = False # Otorga recompensa si el jugador empieza con 21.\n",
    "#                sab = True, # Si el jugador tiene 21 y el dealer también, es un empate. http://www.incompleteideas.net/book/RLbook2020.pdf\n",
    "#                )\n",
    "\n",
    "# env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15, 10, False), {})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from env.blackjack import BlackjackEnv\n",
    "\n",
    "env = BlackjackEnv()\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 : Jugador\n",
      "10 : Dealer\n",
      "False : Hay algún as \"utilizable\"\n"
     ]
    }
   ],
   "source": [
    "observation, info = env.reset(seed = 0)\n",
    "print(observation[0], ': Jugador')\n",
    "print(observation[1], ': Dealer')\n",
    "print(observation[2], ': Hay algún as \"utilizable\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Elegimos una acción aleatoria\n",
    "action = env.action_space.sample()\n",
    "\n",
    "# 0 = stand (nos quedamos con el valor que tenemos)\n",
    "# 1 = hit (pedimos una carta más)\n",
    "\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (11, 19, False) True -1.0\n"
     ]
    }
   ],
   "source": [
    "observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "print(action, observation, terminated, reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<env.blackjack.BlackjackEnv at 0x2872d896590>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent.blackjack import BlackjackAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "n_episodes = 100_000\n",
    "start_epsilon = 1.0\n",
    "epsilon_decay = start_epsilon / (n_episodes / 2)  # reduce the exploration over time\n",
    "final_epsilon = 0.1\n",
    "\n",
    "agent = BlackjackAgent(learning_rate = learning_rate,\n",
    "                       initial_epsilon = start_epsilon,\n",
    "                       epsilon_decay = epsilon_decay,\n",
    "                       final_epsilon = final_epsilon,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<RecordEpisodeStatistics<BlackjackEnv instance>>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ciruz\\OneDrive\\Documentos\\The Bridge\\T.A\\Aprendizaje por Refuerzo\\aprendizaje_por_refuerzo\\Práctica\\qlearning.ipynb Cell 26\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ciruz/OneDrive/Documentos/The%20Bridge/T.A/Aprendizaje%20por%20Refuerzo/aprendizaje_por_refuerzo/Pr%C3%A1ctica/qlearning.ipynb#X34sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# play one episode\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ciruz/OneDrive/Documentos/The%20Bridge/T.A/Aprendizaje%20por%20Refuerzo/aprendizaje_por_refuerzo/Pr%C3%A1ctica/qlearning.ipynb#X34sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ciruz/OneDrive/Documentos/The%20Bridge/T.A/Aprendizaje%20por%20Refuerzo/aprendizaje_por_refuerzo/Pr%C3%A1ctica/qlearning.ipynb#X34sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mget_action(obs)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ciruz/OneDrive/Documentos/The%20Bridge/T.A/Aprendizaje%20por%20Refuerzo/aprendizaje_por_refuerzo/Pr%C3%A1ctica/qlearning.ipynb#X34sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     next_obs, reward, terminated, truncated, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ciruz/OneDrive/Documentos/The%20Bridge/T.A/Aprendizaje%20por%20Refuerzo/aprendizaje_por_refuerzo/Pr%C3%A1ctica/qlearning.ipynb#X34sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39m# update the agent\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ciruz\\OneDrive\\Documentos\\The Bridge\\T.A\\Aprendizaje por Refuerzo\\aprendizaje_por_refuerzo\\Práctica\\agent\\blackjack.py:39\u001b[0m, in \u001b[0;36mBlackjackAgent.get_action\u001b[1;34m(self, obs)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[39m# with probability epsilon return a random action to explore the environment\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandom() \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepsilon:\n\u001b[1;32m---> 39\u001b[0m     \u001b[39mreturn\u001b[39;00m env\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39msample()\n\u001b[0;32m     41\u001b[0m \u001b[39m# with probability (1 - epsilon) act greedily (exploit)\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     43\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mint\u001b[39m(np\u001b[39m.\u001b[39margmax(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_values[obs]))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "env = gym.wrappers.RecordEpisodeStatistics(env, deque_size = n_episodes)\n",
    "for episode in tqdm(range(n_episodes)):\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "\n",
    "    # play one episode\n",
    "    while not done:\n",
    "        action = agent.get_action(obs)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        # update the agent\n",
    "        agent.update(obs, action, reward, terminated, next_obs)\n",
    "\n",
    "        # update if the environment is done and the current obs\n",
    "        done = terminated or truncated\n",
    "        obs = next_obs\n",
    "\n",
    "    agent.decay_epsilon()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_length = 500\n",
    "fig, axs = plt.subplots(ncols = 3, figsize = (12, 5))\n",
    "axs[0].set_title(\"Episode rewards\")\n",
    "# compute and assign a rolling average of the data to provide a smoother graph\n",
    "reward_moving_average = (np.convolve(np.array(env.return_queue).flatten(),\n",
    "                                     np.ones(rolling_length),\n",
    "                                     mode = \"valid\") / rolling_length)\n",
    "axs[0].plot(range(len(reward_moving_average)), reward_moving_average)\n",
    "axs[1].set_title(\"Episode lengths\")\n",
    "length_moving_average = (np.convolve(np.array(env.length_queue).flatten(),\n",
    "                                     np.ones(rolling_length),\n",
    "                                     mode = \"same\") / rolling_length)\n",
    "axs[1].plot(range(len(length_moving_average)),\n",
    "            length_moving_average)\n",
    "axs[2].set_title(\"Training Error\")\n",
    "training_error_moving_average = (np.convolve(np.array(agent.training_error),\n",
    "                                             np.ones(rolling_length),\n",
    "                                             mode = \"same\") / rolling_length)\n",
    "axs[2].plot(range(len(training_error_moving_average)),\n",
    "            training_error_moving_average)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_grids(agent, usable_ace = False):\n",
    "    \"\"\"Create value and policy grid given an agent.\"\"\"\n",
    "    # convert our state-action values to state values\n",
    "    # and build a policy dictionary that maps observations to actions\n",
    "    state_value = defaultdict(float)\n",
    "    policy = defaultdict(int)\n",
    "    for obs, action_values in agent.q_values.items():\n",
    "        state_value[obs] = float(np.max(action_values))\n",
    "        policy[obs] = int(np.argmax(action_values))\n",
    "\n",
    "    player_count, dealer_count = np.meshgrid(\n",
    "        # players count, dealers face-up card\n",
    "        np.arange(12, 22),\n",
    "        np.arange(1, 11))\n",
    "\n",
    "    # create the value grid for plotting\n",
    "    value = np.apply_along_axis(lambda obs: state_value[(obs[0], obs[1], usable_ace)],\n",
    "                                axis = 2,\n",
    "                                arr = np.dstack([player_count, dealer_count]))\n",
    "    value_grid = player_count, dealer_count, value\n",
    "\n",
    "    # create the policy grid for plotting\n",
    "    policy_grid = np.apply_along_axis(lambda obs: policy[(obs[0], obs[1], usable_ace)],\n",
    "                                      axis=2,\n",
    "                                      arr=np.dstack([player_count, dealer_count]))\n",
    "    return value_grid, policy_grid\n",
    "\n",
    "\n",
    "def create_plots(value_grid, policy_grid, title: str):\n",
    "    \"\"\"Creates a plot using a value and policy grid.\"\"\"\n",
    "    # create a new figure with 2 subplots (left: state values, right: policy)\n",
    "    player_count, dealer_count, value = value_grid\n",
    "    fig = plt.figure(figsize=plt.figaspect(0.4))\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "\n",
    "    # plot the state values\n",
    "    ax1 = fig.add_subplot(1, 2, 1, projection=\"3d\")\n",
    "    ax1.plot_surface(player_count,\n",
    "                     dealer_count,\n",
    "                     value,\n",
    "                     rstride=1,\n",
    "                     cstride=1,\n",
    "                     cmap=\"viridis\",\n",
    "                     edgecolor=\"none\",)\n",
    "    plt.xticks(range(12, 22), range(12, 22))\n",
    "    plt.yticks(range(1, 11), [\"A\"] + list(range(2, 11)))\n",
    "    ax1.set_title(f\"State values: {title}\")\n",
    "    ax1.set_xlabel(\"Player sum\")\n",
    "    ax1.set_ylabel(\"Dealer showing\")\n",
    "    ax1.zaxis.set_rotate_label(False)\n",
    "    ax1.set_zlabel(\"Value\", fontsize=14, rotation=90)\n",
    "    ax1.view_init(20, 220)\n",
    "\n",
    "    # plot the policy\n",
    "    fig.add_subplot(1, 2, 2)\n",
    "    ax2 = sns.heatmap(policy_grid, linewidth=0, annot=True, cmap=\"Accent_r\", cbar=False)\n",
    "    ax2.set_title(f\"Policy: {title}\")\n",
    "    ax2.set_xlabel(\"Player sum\")\n",
    "    ax2.set_ylabel(\"Dealer showing\")\n",
    "    ax2.set_xticklabels(range(12, 22))\n",
    "    ax2.set_yticklabels([\"A\"] + list(range(2, 11)), fontsize=12)\n",
    "\n",
    "    # add a legend\n",
    "    legend_elements = [Patch(facecolor=\"lightgreen\", edgecolor=\"black\", label=\"Hit\"),\n",
    "                       Patch(facecolor=\"grey\", edgecolor=\"black\", label=\"Stick\")]\n",
    "    ax2.legend(handles=legend_elements, bbox_to_anchor=(1.3, 1))\n",
    "    return fig\n",
    "\n",
    "\n",
    "# state values & policy with usable ace (ace counts as 11)\n",
    "value_grid, policy_grid = create_grids(agent, usable_ace=True)\n",
    "fig1 = create_plots(value_grid, policy_grid, title=\"With usable ace\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruebas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicialización: Inicializamos la tabla Q, que es una matriz donde las filas representan estados y las columnas representan acciones. Inicialmente, los valores en esta tabla pueden ser aleatorios.\n",
    "\n",
    "Elección de Acción: Para cada episodio, observamos el estado actual y elegimos una acción. La elección de la acción puede ser determinista o basada en una política de exploración, que se introduce con el parámetro epsilon.\n",
    "\n",
    "Interacción con el Entorno: Tomamos la acción elegida y observamos la recompensa del entorno. También observamos el nuevo estado.\n",
    "\n",
    "Actualización de Q-Value: Actualizamos el valor Q para la acción tomada en el estado actual utilizando la fórmula de actualización de Q:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q[state, action] = Q[state, action] + learning_rate * (reward + gamma * np.max(Q[new_state, :]) - Q[state, action])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* learning_rate es la tasa de aprendizaje.\n",
    "* gamma es el factor de descuento que representa la importancia de las recompensas futuras.\n",
    "* Gamma : Este parámetro controla la importancia de las recompensas futuras. Un valor más alto significa que se dará más peso a las recompensas a largo plazo.\n",
    "* Epsilon: Este parámetro controla la probabilidad de elegir una acción aleatoria en lugar de la mejor acción según la tabla Q. A medida que epsilon disminuye con el tiempo, el agente tiende a explotar más y explorar menos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = ['left', 'right']\n",
    "epsilon = 0.9\n",
    "alpha = 0.1\n",
    "gamma = 0.9\n",
    "max_episodes = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_q_table(n_states, actions):\n",
    "    table = pd.DataFrame(np.zeros((n_states, len(actions))),\n",
    "                         columns = actions)\n",
    "    return table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
